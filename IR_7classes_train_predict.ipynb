{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-7classes-train-predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJ55oJT0vnE8+yfq80Wajc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/IR_7classes_train_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpqowEICPpGn"
      },
      "source": [
        "# connect to google drive so that we can save the model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_tPNxafzcjp"
      },
      "source": [
        "# will get raw and segmented images from segments.ai. I am not changing the size of the image for now\n",
        "!pip install --upgrade segments-ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoJlmxPU1waM"
      },
      "source": [
        "from segments import SegmentsClient\n",
        "api_key = \"a89182567b17766b91773021b18d04574cd75109\"\n",
        "client = SegmentsClient(api_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiEf4iBS10Xz"
      },
      "source": [
        "# get the release file\n",
        "dataset_identifier = \"ssnirgudkar/IR-2020-10-22-17-33-25-0\"\n",
        "name = \"V2.0\"\n",
        "release = client.get_release(dataset_identifier, name)\n",
        "print(release)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39bMn_Bo4irh"
      },
      "source": [
        "# export the release. This exports both the raw image and the labeled image \n",
        "from segments import SegmentsDataset\n",
        "from segments.utils import export_dataset\n",
        "dataset = SegmentsDataset(release, labelset='ground-truth', filter_by=['labeled', 'reviewed'])\n",
        "\n",
        "# Export to COCO panoptic format\n",
        "export_dataset(dataset, export_format='coco-panoptic')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5bMlGf_4We"
      },
      "source": [
        "# code to convert the instance ids to category ids\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "WASR_SKY_ID = 2\n",
        "WASR_WATER_ID = 1\n",
        "WASR_OBSTACLE_ID = 0\n",
        "MAX_ID = WASR_SKY_ID+1 # This can change in future.\n",
        "# These labels should be read from JSON file but for now are hard-coded.\n",
        "OUR_SKY_ID = 0\n",
        "OUR_WATER_ID = 1\n",
        "OUR_STRUCTURE_ID = 2\n",
        "OUR_OBSTACLE_ID = 3\n",
        "OUR_LIVING_OBSTACLE_ID = 4\n",
        "OUR_BACKGROUND_ID = 5\n",
        "OUR_SELF_ID = 6\n",
        "\n",
        "TOTAL_INSTANCES = 20\n",
        "\n",
        "# userlabel can be anything such as 'ground-truth' or 'segmentation'\n",
        "\n",
        "def readJSONFile(jsonFilePath, userlabel):\n",
        "    fileH = open(jsonFilePath)\n",
        "    data = json.load(fileH)\n",
        "\n",
        "    #print(data)\n",
        "    dictImageRemap = {}\n",
        "    \n",
        "    for index in data['dataset']['samples']:\n",
        "        #print(\"record of = {0}\".format(index['name']))\n",
        "        # The instance ids 'id' in JSON file can technically have no upper bound. This is because\n",
        "        # if there are number of objects in the scene, instance ids will just keep increasing.\n",
        "        # So the below number needs to be adjusted depending on what you find in JSON file.\n",
        "        # If JSON file has larger number in its 'id' field than below then adjust the number below\n",
        "        # to match the high number in JSON file. Specifically it is 'number+1'.\n",
        "        mapIds = [None] * TOTAL_INSTANCES\n",
        "        #print(index['name'], index['labels']['ground-truth']['attributes']['annotations'])\n",
        "        # if images are not labeled then JSON file will contain \n",
        "        # ground-truth = null. Handle the case of if index['labels']['ground-truth'] exists\n",
        "        if index['labels'][userlabel] is not None:\n",
        "            #print(\"len(index['labels']['ground-truth'])={0}\".format(len(index['labels']['ground-truth'])))\n",
        "            if 'attributes' in index['labels'][userlabel]:\n",
        "                if len(index['labels'][userlabel]['attributes']['annotations']) > 0:\n",
        "                    #print(i, index['labels']['ground-truth']['attributes']['annotations'][i]['category_id'], index['labels']['ground-truth']['attributes']['annotations'][i]['id'])\n",
        "                    for jj in range(len(index['labels'][userlabel]['attributes']['annotations'])):\n",
        "                        #print(\"id={0} will be replaced by\".format(index['labels']['ground-truth']['attributes']['annotations'][jj]['id']))\n",
        "                        #print(\"category_id={0}\".format(index['labels'][userlabel]['attributes']['annotations'][jj]['category_id']))\n",
        "                        #print(\"index['labels'][userlabel]['attributes']['annotations'][jj]['id']={0}\".format(index['labels'][userlabel]['attributes']['annotations'][jj]['id']))\n",
        "                        #print(\"index['labels'][userlabel]['attributes']['annotations'][jj]['category_id']={0}\".format(index['labels'][userlabel]['attributes']['annotations'][jj]['category_id']))\n",
        "                        mapIds[index['labels'][userlabel]['attributes']['annotations'][jj]['id']] = index['labels'][userlabel]['attributes']['annotations'][jj]['category_id']\n",
        "                    #print(\"mapIds={0}\".format(mapIds))    \n",
        "        dictImageRemap[index['name']] = mapIds\n",
        "        #print(\"dictImageRemap[index['name']]={0}\".format(dictImageRemap[index['name']]))\n",
        "\n",
        "    fileH.close()\n",
        "    return dictImageRemap\n",
        "\n",
        "# This function creates 2 mask image files for a single input mask image.\n",
        "# First conversion is from segments.ai's instance segmentation map to semantic segmentation map\n",
        "# Second conversion is from semantic segmentation map to wasr 3 classes map.\n",
        "# By saving 2 separate mask images, it is easy to carry out experimentation.\n",
        "# It is expected that the input files are resized appropriately by running imageutilities.py\n",
        "# script so that its name conforms to the naming convention i.e. the file name contains \"_\".\n",
        "def change_one_label_with_another_label(fileOrDirName, records):  \n",
        "\n",
        "    def change_label(filename, mapRelabel):\n",
        "        img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
        "        if (img is None):\n",
        "            print(\"Image = {0} does not exist.\".format(filename))\n",
        "        \n",
        "        unique_values_img = np.unique(img)\n",
        "        #print(\"Unique label ids in img = {0}\".format(unique_values_img))\n",
        "\n",
        "        img2 = img.copy()\n",
        "        if len(img.shape) != 2:\n",
        "            print(\"Skipping {0} because it contains channel information, mask image should be 2-D only.\".format(filename))\n",
        "            return\n",
        "\n",
        "        height,width = img.shape\n",
        "\n",
        "        dir_filename_pair = os.path.split(filename)\n",
        "        #print(\"dir_filename_pair={0}\".format(dir_filename_pair))\n",
        "        ext = dir_filename_pair[-1][-4:]\n",
        "        dirName = dir_filename_pair[:-1]\n",
        "        #print(\"dir={0}\".format(dirName[0]))\n",
        "        oneDirUp = os.path.abspath(os.path.join(dirName[0], os.pardir))\n",
        "        #print(\"oneDirUp={0}\".format(oneDirUp))\n",
        "        inputFileName = dir_filename_pair[-1]\n",
        "        #print(\"inputFileName={0}\".format(inputFileName))\n",
        "        tokens2 = inputFileName.split('_')\n",
        "        #print(\"tokens2={0}\".format(tokens2))\n",
        "        origImageName = tokens2[0] + ext\n",
        "        #print(\"origImageName={0}\".format(origImageName))\n",
        "        rawfileName = dir_filename_pair[-1][:-4] # last token (-1) is full file name. (-4) is without \".png\"\n",
        "\n",
        "        \n",
        "        semantic_segment_dir_name = os.path.join(oneDirUp, 'segment_relabel_semantic')\n",
        "        #print(\"semantic_segment_dir_name={0}\".format(semantic_segment_dir_name))\n",
        "        semantic_segment_dir = semantic_segment_dir_name\n",
        "        if not os.path.isdir(semantic_segment_dir_name):\n",
        "            os.mkdir(semantic_segment_dir_name)\n",
        "            #print(\"semantic_segment_dir={0}\".format(semantic_segment_dir))\n",
        "        \n",
        "        outputfilename2 = rawfileName + \"_relabeled_semantically\" + ext\n",
        "        fulloutputfilename2 = os.path.join(semantic_segment_dir, outputfilename2)\n",
        "\n",
        "        #print(\"input file = {0}\".format(inputFileName))\n",
        "        mapOfThisFile = records[origImageName]\n",
        "        #print(\"file={1}, mapOfThisFile={0}\".format(mapOfThisFile, inputFileName))\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "                for k in range(len(mapOfThisFile)):\n",
        "                    if ((img.item(i,j) == k) and (mapOfThisFile[k] is not None)):\n",
        "                        #print(\"i={3},j={4},img.item(i,j)={0}, k={1}, mapOfThisFile[k]={2}\".format(img.item(i,j), k, mapOfThisFile[k],i,j))\n",
        "                        img2.itemset((i,j), mapOfThisFile[k])\n",
        "\n",
        "        cv2.imwrite(fulloutputfilename2, img2)\n",
        "\n",
        "        ''' this is for 3 classes. hence commented \n",
        "        unique_values_img2 = np.unique(img2)\n",
        "        #print(\"Unique label ids in img2 = {0}\".format(unique_values_img2))\n",
        "\n",
        "        relabel2_wasr3_dir_name = os.path.join(oneDirUp, 'segment_relabel2_wasr3')\n",
        "        relabel2_wasr3_dir = relabel2_wasr3_dir_name\n",
        "        if not os.path.isdir(relabel2_wasr3_dir_name):\n",
        "            os.mkdir(relabel2_wasr3_dir_name)\n",
        "\n",
        "        outputfilename3 = rawfileName + \"_relabeled_to3wasrclasses\" + ext\n",
        "        fulloutputfilename3 = os.path.join(relabel2_wasr3_dir, outputfilename3)\n",
        "        our2WasrMap = [None] * 8\n",
        "        our2WasrMap[OUR_SKY_ID] = WASR_SKY_ID\n",
        "        our2WasrMap[OUR_WATER_ID] = WASR_WATER_ID\n",
        "        our2WasrMap[OUR_STRUCTURE_ID] = WASR_OBSTACLE_ID\n",
        "        our2WasrMap[OUR_OBSTACLE_ID] = WASR_OBSTACLE_ID\n",
        "        # Keep the remaining labels intact\n",
        "        our2WasrMap[OUR_LIVING_OBSTACLE_ID] = our2WasrMap[OUR_BACKGROUND_ID] = our2WasrMap[OUR_SELF_ID] = WASR_OBSTACLE_ID\n",
        "        img3 = img2.copy()\n",
        "\n",
        "        #print(\"ours2Wasrmap={0}\".format(our2WasrMap))\n",
        "\n",
        "        for i in range(height):\n",
        "            for j in range(width):\n",
        "                for k in range(len(our2WasrMap)):\n",
        "                    if ((img2.item(i,j) == k) and (our2WasrMap[k] is not None)):\n",
        "                        #print(\"img2.item(i,j)={0}, k={1}, our2WasrMap[k]={2}\".format(img2.item(i,j), k, our2WasrMap[k]))\n",
        "                        img3.itemset((i,j), our2WasrMap[k])\n",
        "\n",
        "\n",
        "        unique_values_img3 = np.unique(img3)\n",
        "        print(\"Unique label ids in img3 = {0}\".format(unique_values_img3))\n",
        "        maxId = max(unique_values_img3)\n",
        "        if (maxId >= MAX_ID):\n",
        "            raise Exception(\"Conversion not correct for file:{0}\".format(filename))\n",
        "\n",
        "        cv2.imwrite(fulloutputfilename3, img3)            \n",
        "        '''\n",
        "    if (os.path.isdir(fileOrDirName)):\n",
        "        for eachFile in os.listdir(fileOrDirName):\n",
        "            if (eachFile.endswith(\"ground-truth.png\") or eachFile.endswith(\".jpg\")):\n",
        "                eachFileFullPath = os.path.join(fileOrDirName, eachFile)\n",
        "                print(\"eachFile = {0}\".format(eachFileFullPath))\n",
        "                change_label(eachFileFullPath, records)\n",
        "    else:\n",
        "        change_label(fileOrDirName, records)    \n",
        "\n",
        "print(\"calling read json file\")     \n",
        "records = readJSONFile(\"/content/IR-2020-10-22-17-33-25-0-V2.0.json\", \"ground-truth\")\n",
        "print(\"calling label change\")\n",
        "change_one_label_with_another_label(\"/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0\", records)\n",
        "print(\"end change label\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odlh-TPw7VEn"
      },
      "source": [
        "Because the trainning code is for classes and different image size, I will create new training code here. Above the pixel values have been changed to category ids. Size of the image is same as that in segments.ai \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjEtX5-86WwF",
        "outputId": "1dcb7096-a692-4a40-bb46-896356a2b7cd"
      },
      "source": [
        "# read the downloaded files and check the pixel values to make sure they are right\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "input_dir = \"/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0\" # directory containing input raw images\n",
        "target_dir = \"/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic\" # directory containing labeled images\n",
        "img_size = (512, 640) #it's height and width. it's a 1/2 sized image from the original image from segment.ai\n",
        "num_classes = 7\n",
        "batch_size = 5\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".png\") and (\"ground-truth\" not in fname)\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\"semantically.png\")  \n",
        "    ]\n",
        ")\n",
        "\n",
        "#prints how many files are in the batch\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "#prints the name of 10 sets of input and labeled file\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)\n",
        "\n",
        "#below code checks if the labels are in the 1 through 3 range (4 classes - sky,water,object,background). \n",
        "#Because if not, then model training gives a NAN error.\n",
        "#it displays the label file that has an error, and the values of the label\n",
        "ideal_label_array = [0,1,2,3,4,5,6] # depends on the number of classes\n",
        "flag = 0 \n",
        "for target_path in (target_img_paths):\n",
        "    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\n",
        "    if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "       flag +=1\n",
        "       #print(\"good label\", flag)\n",
        "    else:    \n",
        "       flag +=1\n",
        "       print(\"Error in label\", flag,target_path,target_label_array )  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 62\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388082.940481.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388082.940481_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388099.294270.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388099.294270_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388115.497458.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388115.497458_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388165.895730.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388165.895730_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388188.478616.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388188.478616_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388213.024487.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388213.024487_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388233.503927.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388233.503927_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388257.753172.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388257.753172_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388265.894147.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388265.894147_label_ground-truth_relabeled_semantically.png\n",
            "/content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/V2.0/1603388285.976467.png | /content/segments/ssnirgudkar_IR-2020-10-22-17-33-25-0/segment_relabel_semantic/1603388285.976467_label_ground-truth_relabeled_semantically.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdj8066HX0R"
      },
      "source": [
        "Prepare Sequence class to load & vectorize batches of data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAyjP0-lHc-t"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            #in case we want to see if masked image have the right values\n",
        "            #print(tensorflow.keras.preprocessing.image.img_to_array(img))\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2: \n",
        "            #if classes are 3, keras expects masked values to be 0,1,2 only. Cannot take 1,3,5. It checks numerically not just the number of masked values\n",
        "            #y[j] += 1 \n",
        "        return x, y\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUVc4usZHrsY"
      },
      "source": [
        "## Prepare U-Net Xception-style model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjrJQzINHtEI",
        "outputId": "76c6c2a6-612b-4af6-bded-5e4062651ec8"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2 ,padding=\"same\",kernel_initializer = 'uniform')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"ReLU\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'uniform')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'uniform')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\", )(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\",  kernel_initializer = 'uniform')(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1,padding=\"same\", kernel_initializer = 'uniform')(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\",kernel_initializer = 'uniform')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 512, 640, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 256, 320, 32) 896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 256, 320, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 256, 320, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 256, 320, 32) 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 256, 320, 64) 2400        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256, 320, 64) 256         separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 256, 320, 64) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 256, 320, 64) 4736        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 256, 320, 64) 256         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 128, 160, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 160, 64) 2112        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 128, 160, 64) 0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 128, 160, 64) 0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 128, 160, 128 8896        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 160, 128 512         separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 128, 160, 128 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 128, 160, 128 17664       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 128, 160, 128 512         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 80, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 80, 128)  8320        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 80, 128)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 80, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 64, 80, 256)  34176       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 80, 256)  1024        separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 80, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 64, 80, 256)  68096       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 80, 256)  1024        separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 40, 256)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 40, 256)  33024       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 40, 256)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 40, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 32, 40, 256)  590080      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 40, 256)  1024        conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 40, 256)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 32, 40, 256)  590080      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 40, 256)  1024        conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 64, 80, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 64, 80, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 80, 256)  65792       up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64, 80, 256)  0           up_sampling2d[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 64, 80, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 64, 80, 128)  295040      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 64, 80, 128)  512         conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 64, 80, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 64, 80, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 64, 80, 128)  512         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 128, 160, 256 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 128, 160, 128 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 128, 160, 128 32896       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 128, 160, 128 0           up_sampling2d_2[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 128, 160, 128 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 128, 160, 64) 73792       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 128, 160, 64) 256         conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 128, 160, 64) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 128, 160, 64) 36928       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 128, 160, 64) 256         conv2d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 256, 320, 128 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 256, 320, 64) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 256, 320, 64) 8256        up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 256, 320, 64) 0           up_sampling2d_4[0][0]            \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 256, 320, 64) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 256, 320, 32) 18464       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 256, 320, 32) 128         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 256, 320, 32) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 256, 320, 32) 9248        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 256, 320, 32) 128         conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 512, 640, 64) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 512, 640, 32) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 512, 640, 32) 2080        up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 512, 640, 32) 0           up_sampling2d_6[0][0]            \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 512, 640, 7)  2023        add_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 2,060,135\n",
            "Trainable params: 2,056,359\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhbgZO19IN-1"
      },
      "source": [
        "## Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRqTD6M_IOtZ"
      },
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 5\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = OxfordPets(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRsmAT8jIUtj"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU6AY2PsIVU7"
      },
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "\n",
        "import tensorflow as tf\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "#model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    #commenting checkpoint because i am going to save the entire model and start with earlier model\n",
        "    #keras.callbacks.ModelCheckpoint(\"IR_1000images_he_normal_adam_softmax_relu_300_10_1.h5\", save_best_only=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch. This will be the first time code. Thereafter, you first load the saved model and then train again\n",
        "epochs = 100\n",
        "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n",
        "\n",
        "# Save the model after training \n",
        "model.save(\"/content/drive/MyDrive/Models/UNet_7classes_1.0\")\n",
        "\n",
        "# now load the model for the next run \n",
        "#reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/UNet1.0\")\n",
        "\n",
        "# now re-train on the saved model - this will be from run2\n",
        "#epochs = 100\n",
        "#reconstructed_model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXK81IQ3JNcg"
      },
      "source": [
        "## Train the Model - Run 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2-b9b9oJORE"
      },
      "source": [
        "# after the 1st run, we can just run from this step on (excluding the files etc)\n",
        "\n",
        "# now load the model for the next run \n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/UNet1.0\")\n",
        "\n",
        "# now re-train on the saved model - this will be from run2\n",
        "epochs = 1\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "reconstructed_model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n",
        "\n",
        "# now save the model back ### REMEMBER : ONLY save the modle if you know your run is with good data. \n",
        "# ELSE it will spoil all the prior learning\n",
        "reconstructed_model.save(\"/content/drive/MyDrive/Models/UNet_7classes_1.0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpnlgGXpJYdk"
      },
      "source": [
        "##Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjbFPHd4JZG6"
      },
      "source": [
        "print(val_input_img_paths)\n",
        "print(val_target_img_paths)\n",
        "print(type(val_input_img_paths))\n",
        "\n",
        "# Generate predictions for all images in the validation set\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "#from PIL import Image\n",
        "\n",
        "#val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "#val_input_img_paths_test = ['/content/drive/MyDrive/poorlighting_brightlight_resized/1603208980.690948_resized.png']\n",
        "val_input_img_paths_test = ['/content/drive/MyDrive/poorlighting_brightlight_resized/1603208980.690948_resized.png']\n",
        "val_target_img_paths_test = ['/content/drive/MyDrive/IRImageInferenceDataset/IR-labeled-image/1622942984.171934_label_ground-truth_resized_relabeled_to3wasrclasses.png']\n",
        "\n",
        "\n",
        "#print(type(val_input_img_paths_test))\n",
        "\n",
        "val_gen = OxfordPets(1, img_size, val_input_img_paths_test, val_target_img_paths_test)\n",
        "# we will use model for the 1st run. and then reconstructed_model from run 2\n",
        "#val_preds = model.predict(val_gen)\n",
        "\n",
        "#predict using the saved model - run 2 on \n",
        "val_preds = reconstructed_model.predict(val_gen)\n",
        "\n",
        "\n",
        "def display_mask(i):\n",
        "    '''Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "    OUR_SKY_ID = 0.  - deep sky blue - (0,191,255)  \n",
        "    OUR_WATER_ID = 1.  - aqua marine - (127,255,212)\n",
        "    OUR_STRUCTURE_ID = 2 - indian red - (205,92,92)\n",
        "    OUR_OBSTACLE_ID = 3 - deep pink - (255,20,147)\n",
        "    OUR_LIVING_OBSTACLE_ID = 4 - lime green -(50,205,50) \n",
        "    OUR_BACKGROUND_ID = 5 - sandy brown - (244,164,96)\n",
        "    OUR_SELF_ID = 6 - thistle - (216,191,216)\n",
        "    '''\n",
        "    label_colours = [(0,191,255), (127,255,212), (205,92,92), \n",
        "                     (255,20,147), (50,205,50), (244,164,96), (216,191,216)]  \n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    print(np.shape(mask))\n",
        "    print(len(mask[i, 0]))\n",
        "    print(len(mask[i]))\n",
        "\n",
        "    img1 = PIL.Image.new('RGB', (640, 512))\n",
        "    pixels = img1.load()\n",
        "    print(type(pixels))\n",
        "    print(pixels[0,0])\n",
        "    for j_, j in enumerate(mask[:, :, 0]):\n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "              #print(k_, k)\n",
        "              if k < num_classes:\n",
        "                  pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "    \n",
        "    print(\"this is the colored inferred image\")\n",
        "    display(img1)\n",
        "  \n",
        "\n",
        "    print(\"this is the gray inferred image\")\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "\n",
        "# Display results for validation image #40. it uses val_inpit_images. so will only pick from the ones that we have kept for validation.\n",
        "i = 0\n",
        "\n",
        "# Display input image\n",
        "print(\"this is the raw image\")\n",
        "display(Image(filename=val_input_img_paths_test[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "print(\"this is the labeled image\")\n",
        "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths_test[i]))\n",
        "display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i)  # Note that the model only sees inputs at 150x150."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}