{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_200 images thru UNET",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/IR_200_images_thru_UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o_-p5pVmn6r"
      },
      "source": [
        "1. no of classes if mentioned 3, then the labels **can only** have values of 0,1 and 2\n",
        "2. You can check the actual error using CPU, and not GPU\n",
        "3. softmax is good for image segmentation. it's a probablistic algorithm, so the losses also haev to be accordingly. all of the below are with softmax activation. optimizers are changed in the middle.  \n",
        "4. relu / leakyrelu, adam will work \n",
        "5. **Option 1-** leakyrelu - epoc 50, 62 images of which validation images 3. batch size 3 , kernel_initializer = 'he_normal'- , leakyrelu, sparse_categorical_crossentropy, optimizer - rmsprop: val_loss went from 87 to 0.2. But it was not continually reducing and was fluctutating.  Output segmentation was good. obstacles were showing up.\n",
        "6. **Option 2- **RELU, epoc 50, total images - 62, validation 3 images, sparse_categorical_crossentropy, kernel_initializer = 'he_normal', optimizer = rmsprop: Val_loss started from 127 and went to 0.08. was better converging than option 1, but still not always decreasing. Image output better than option 1\n",
        "7. Option 3: **RELU, epoc 50, total images - 62, validation 3 images, sparse_categorical_crossentropy, kernel_initializer = 'he_normal', optimizer = adam, defalt learn rate(lr): Val_loss started from 45 and went to 0.0528. The val loss continually decreased and converged. Image output better than option 1 and 2. Observations - reflection in water is shown as an obstacle. Bits of water are shown as cloud\n",
        "8. Option 4: **RELU, epoc 50, total images - 62, validation 3 images, categorical_crossentropy, kernel_initializer = 'he_normal', optimizer - adam, lr = 0.01: Val_loss started from 476 billion  and went to NAN in the 6th apoch. No segmentation output available. it was all black. Tried to change the loss to sparse_categorical_crossentropy with the same lr - 0.01, but the op was worse than that with default lr = 0.001  \n",
        "Kept the loss to sparse_categorical_crossentropy , but changed the lr to 0.005. That works comparable to lr = 0.001(default), but a touch lower. Some images are at par with default lr, some are not.   \n",
        "9. adam, lr = 0.001(default), softmax, kernel_initializer = 'uniform'. val_loss starts from 2.4 to 0.06. Some images with this are better than those using he_normal. \n",
        "**Recommendation - adam, sparse_categorical_crossentropy, relu, lr=. default = 0.001, kernal initilizer = 'he_normal' OR kernal initilizer = 'uniform'**\n",
        "\n",
        "Training results - \n",
        "1. Ran with 291 IR images (including night _ pilot IR. Mirror images of those + labeled) = 882 images in this directory. The checkpoint file is IR_1000images_uniform_adam_softmax_100_10.h5. Indictes, it was run for 1000 images, uniform kernel initializer, adam optimizer, softmax algo, 100 epochs with a batch size of 10. No of classes are 4 (0 thru 3). This was run with TPU. Per Epoc (86 batches with a batch size of 10 - estimate time was 10 min). Val_loss for epoc 1 was 5.0147 although average loss was 0.3 for each batch. Then ran with GPU to see which one to go with. GPU checking the unique label value took lot of time for 800 images. However, the training time - 1 epoch 1st batch with 86 batches with a batch size of 10 - estimate was only 2 min. Val loss was 7 although loss was 0.1. So GPU was much faster for training than TPU.Also TPU libraries to be imported change, plus TPU gives verbose explanbation of error as against GPU that just says NAN\n",
        "\n",
        "Results - \n",
        "Run1: 676 images with GPU and above configuration took 28 min - and val_loss was 0.0708 after 100 epocs. No of classes -3 , batchsize 10. Model uploaded to my google drive - https://drive.google.com/file/d/1bP6M-6w-dMHKRq3EbXULGATXGlLsdS6e/view?usp=sharing\n",
        "Run2 - same as run1. But added tensorboard callbacks. Model op file name -  IR_1000images_uniform_adam_softmax_relu_100_10_1.h5. var_loss - 0.0618\n",
        "\n",
        "Inferecnce - Night time images (loss = 0.0147) - for paper on July end \n",
        "1. Ran 3 night time images that were segmented so that we can compare the groundtruth with inference. 4 IR images that were not segmented, so that we can see their inference straight. And then all these 7 optical color images. All were resized. - /content/drive/MyDrive/poorlighting_nighttime_resized\n",
        "\n",
        "2. Ran 5 optical images where there is bright sunlight. these were not used in segmentation - /content/drive/MyDrive/poorlighting_brightlight_resized. Val loss - 0.0195\n",
        "\n",
        "3. Updated the inferred image from gray to color - so that it can be used in papers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754aS1YJgOVn"
      },
      "source": [
        "# Image segmentation with a U-Net-like architecture\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/03/20<br>\n",
        "**Last modified:** 2020/04/20<br>\n",
        "**Description:** Image segmentation model trained from scratch on the Oxford Pets dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQcQdRZQgOVo"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTVPl0nyi9Ms"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd \"drive/My Drive/PhD/IRLabeledDataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQgZp6JWgOVp"
      },
      "source": [
        "## Prepare paths of input images and target segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKBranBjhihg"
      },
      "source": [
        "'''this is a test code to check syntax \n",
        "flag = 0 \n",
        "ideal_label_array = [0,1,2,3]\n",
        "target_label_array = [0,1,2,3,5]\n",
        "if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "  flag = 0\n",
        "else:    \n",
        "  flag = 1 \n",
        "  print(\"Error in label\", target_label_array ) \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGXBC0LgOVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b744d150-887a-4cfc-f166-56d9bd3cb658"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "'''\n",
        "older ir images from shailesh's drive\n",
        "input_dir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized\" # directory containing input images\n",
        "target_dir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized_relabel_only3wasrclasses\" # directory containing segmented images\n",
        "'''\n",
        "input_dir = \"/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images\" # directory containing input images\n",
        "target_dir = \"/content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images\" # directory containing segmented images\n",
        "\n",
        "\n",
        "img_size = (256, 320) #it's height and width. it's a 1/2 sized image from the original image from segment.ai\n",
        "num_classes = 3\n",
        "batch_size = 10\n",
        "\n",
        "# check for distinct label values in all masked files \n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".png\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "#prints how many files are in the batch\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "#prints the name of 10 sets of input and labeled file\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)\n",
        "\n",
        "#below code checks if the labels are in the 1 through 3 range (4 classes - sky,water,object,background). \n",
        "#Because if not, then model training gives a NAN error.\n",
        "#it displays the label file that has an error, and the values of the label\n",
        "ideal_label_array = [0,1,2]\n",
        "flag = 0 \n",
        "for target_path in (target_img_paths):\n",
        "    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\n",
        "    if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "       flag +=1\n",
        "       #print(\"good label\", flag)\n",
        "    else:    \n",
        "       flag +=1\n",
        "       print(\"Error in label\", flag,target_path,target_label_array )  \n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 670\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161174.581624_resized_color.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161174.581624_label_ground-truth_resized_relabeled_to3wasrclasses.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161174.581624_resized_color_mirrored.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161174.581624_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161258.414889_resized_color.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161258.414889_label_ground-truth_resized_relabeled_to3wasrclasses.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161258.414889_resized_color_mirrored.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161258.414889_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161267.781607_resized_color.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161267.781607_label_ground-truth_resized_relabeled_to3wasrclasses.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161267.781607_resized_color_mirrored.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161267.781607_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161378.948094_resized_color.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161378.948094_label_ground-truth_resized_relabeled_to3wasrclasses.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161378.948094_resized_color_mirrored.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161378.948094_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161440.714693_resized_color.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161440.714693_label_ground-truth_resized_relabeled_to3wasrclasses.png\n",
            "/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161440.714693_resized_color_mirrored.png | /content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161440.714693_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZxuEZuOgOVq"
      },
      "source": [
        "## What does one input image and corresponding segmentation mask look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qTygTjxLB_M"
      },
      "source": [
        "import cv2\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import PIL\n",
        "\n",
        "img = cv2.imread('/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161174.581624_resized_color_mirrored.png')\n",
        "print(img) \n",
        "img1 = cv2.imread('/content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161174.581624_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png', cv2.IMREAD_UNCHANGED)\n",
        "print(img1)\n",
        "print(img.shape) # height , width, color\n",
        "print(img1.shape)\n",
        "array1 = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
        "print(array1) \n",
        "array2 = tensorflow.keras.preprocessing.image.img_to_array(img1)\n",
        "print(array2) \n",
        "print(np.unique(array1))\n",
        "print(np.unique(array2))\n",
        "print(tensorflow.keras.backend.epsilon())\n",
        "print(np.unique(cv2.imread('/content/drive/MyDrive/IRImageTrainingDataset/ir_train_images_resized_colored_291_images/1571161174.581624_resized_color_mirrored.png')))\n",
        "print(np.unique(cv2.imread('/content/drive/MyDrive/IRImageTrainingDataset/ir_train_masks_resized_relabeled_291_images/1571161174.581624_label_ground-truth_resized_relabeled_to3wasrclasses_mirrored.png',cv2.IMREAD_UNCHANGED)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ucoLp1UgOVq"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #9\n",
        "display(Image(filename=input_img_paths[9]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26N4oURgOVq"
      },
      "source": [
        "## Prepare `Sequence` class to load & vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvauKhqOgOVr"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            #in case we want to see if masked image have the right values\n",
        "            #print(tensorflow.keras.preprocessing.image.img_to_array(img))\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2: \n",
        "            #if classes are 3, keras expects masked values to be 0,1,2 only. Cannot take 1,3,5. It checks numerically not just the number of masked values\n",
        "            #y[j] += 1 \n",
        "        return x, y\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O844f7oDgOVr"
      },
      "source": [
        "## Prepare U-Net Xception-style model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-pfuOspgOVr"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2 ,padding=\"same\",kernel_initializer = 'he_normal')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"ReLU\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'he_normal')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'he_normal')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\", )(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\",  kernel_initializer = 'he_normal')(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1,padding=\"same\", kernel_initializer = 'he_normal')(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\",kernel_initializer = 'he_normal')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D88fKSmGgOVs"
      },
      "source": [
        "## Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eukoj81gOVs"
      },
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 5\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = OxfordPets(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9yf05kfgOVs"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Mgsn2PgOVt"
      },
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "\n",
        "import tensorflow as tf\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "#model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"IR_1000images_he_normal_adam_softmax_relu_100_10_1.h5\", save_best_only=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs', write_graph = False, profile_batch = 0)\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 300\n",
        "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gYCGN4gOVt"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ej7kljfMbOD"
      },
      "source": [
        "print(val_input_img_paths)\n",
        "print(val_target_img_paths)\n",
        "print(type(val_input_img_paths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u11wWRX4gOVt"
      },
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "#from PIL import Image\n",
        "\n",
        "#val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "#val_input_img_paths_test = ['/content/drive/MyDrive/poorlighting_brightlight_resized/1603208980.690948_resized.png']\n",
        "val_input_img_paths_test = ['/content/drive/MyDrive/IRImageInferenceDataset/IR-raw-image/1603209288.988818_resized_color.png']\n",
        "val_target_img_paths_test = ['/content/drive/MyDrive/poorlighting_brightlight_resized/1603209039.994449_resized.png']\n",
        "\n",
        "\n",
        "#print(type(val_input_img_paths_test))\n",
        "\n",
        "val_gen = OxfordPets(1, img_size, val_input_img_paths_test, val_target_img_paths_test)\n",
        "val_preds = model.predict(val_gen)\n",
        "\n",
        "\n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "    #label_colours = [(59,193,246), (222,168,51), (161,78,69), (177, 168, 52), (238, 159, 231), (240, 244, 74), (160, 253, 134)]  \n",
        "    label_colours = [(246,193,59), (51,168,222), (69,78,161), (52, 168, 177), (231, 159, 238), (74, 244, 240), (134, 253, 160)]  \n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    print(np.shape(mask))\n",
        "    print(len(mask[i, 0]))\n",
        "    print(len(mask[i]))\n",
        "\n",
        "    img1 = PIL.Image.new('RGB', (320, 256))\n",
        "    pixels = img1.load()\n",
        "    print(type(pixels))\n",
        "    print(pixels[0,0])\n",
        "    for j_, j in enumerate(mask[:, :, 0]):\n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "              #print(k_, k)\n",
        "              if k < num_classes:\n",
        "                  pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "    \n",
        "    print(\"this is the colored inferred image\")\n",
        "    display(img1)\n",
        "  \n",
        "\n",
        "    print(\"this is the gray inferred image\")\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "\n",
        "# Display results for validation image #40. it uses val_inpit_images. so will only pick from the ones that we have kept for validation.\n",
        "i = 0\n",
        "\n",
        "# Display input image\n",
        "print(\"this is the raw image\")\n",
        "display(Image(filename=val_input_img_paths_test[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "print(\"this is the labeled image\")\n",
        "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths_test[i]))\n",
        "display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i)  # Note that the model only sees inputs at 150x150."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY8aN3EA3bCw"
      },
      "source": [
        "#visualize the architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=1, activation='relu'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "plot_model(model, to_file='/content/IR_1000images_uniform_adam_softmax_relu_100_10-h5-modelplot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkR-8Tu4IVb"
      },
      "source": [
        "# to evaluate the weight tensors\n",
        "from keras import backend as K\n",
        "\n",
        "for w in model.trainable_weights:\n",
        "    print(K.eval(w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkxOGK0G5EYh"
      },
      "source": [
        "# visualize the model in tensorboard - we will use the log files created in checkpoint  \n",
        "\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "%tensorboard --logdir logs\n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}