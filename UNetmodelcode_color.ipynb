{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNetmodelcode_color.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNs0ZQR/h1tDAtpVqkxa0pH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/UNetmodelcode_color.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fVdvrS-DZq4",
        "outputId": "158729f7-3fc7-4393-8792-21541894da44"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd \"drive/My Drive/PhD/IRLabeledDataset\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzUKnp_LBmVg"
      },
      "source": [
        "\n",
        "# CreateImageDirPaths.py\n",
        "import os\n",
        "\n",
        "inputDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized_colored\" # directory containing input colored images (converted from grayscale)\n",
        "segmentationImageDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized\" # directory containing output segmentation images\n",
        "\n",
        "\n",
        "def createImageDirPaths():\n",
        "\n",
        "    if (not inputDir):\n",
        "        print(\"The input dir is empty\")\n",
        "    if(not segmentationImageDir):\n",
        "        print(\"The segmentation dir is empty\")\n",
        "\n",
        "    input_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(inputDir, fName)\n",
        "            for fName in os.listdir(inputDir)\n",
        "                if fName.endswith(\".png\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    target_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(segmentationImageDir, fName)\n",
        "            for fName in os.listdir(segmentationImageDir)\n",
        "                if fName.endswith(\".png\") and not fName.startswith(\".\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Number of samples = {0}\".format(len(input_img_paths)))\n",
        "\n",
        "    for input_path, segmentation_mask_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "        print(input_path, \"|\", segmentation_mask_path)\n",
        "\n",
        "    return input_img_paths, target_img_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLkKL5vmCXYN"
      },
      "source": [
        "# DataSequence.py\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "class MarineImages(keras.utils.Sequence):\n",
        "    \"\"\" Helper to iterate over data (as Numpy array) \"\"\"\n",
        "\n",
        "    def __init__(self, batchSize, imageSize, input_img_paths, segmentation_img_paths):\n",
        "        self.batchSize = batchSize\n",
        "        self.imageSize = imageSize\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.segmentation_img_paths = segmentation_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segmentation_img_paths) // self.batchSize\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Returns tuple (input, segmentation mask) corresponding to batch #idx \"\"\"\n",
        "\n",
        "        i = idx * self.batchSize\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batchSize]\n",
        "        batch_segmentation_img_paths = self.segmentation_img_paths[i : i + self.batchSize]\n",
        "        # use 3 & float32 as input image is RGB (converted from grayscale).  \n",
        "        x = np.zeros((self.batchSize,) + self.imageSize + (3,), dtype=\"float32\")\n",
        "        print(\"x size = {0}\".format(x.shape))\n",
        "\n",
        "        for index, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize, color_mode = 'rgb')\n",
        "            print(\"index = {0}, path = {1}\".format(index, path))\n",
        "            print(\"target size = {0}\".format(self.imageSize))\n",
        "            print(\"img shape in PIL format = {0}\".format(img.size))\n",
        "            print(\"shape of x[{0}] = {1}\".format(index, x[index].shape))\n",
        "            x[index] = img\n",
        " \n",
        "        # use 1 & uint8 as labled image is also grayscale \n",
        "        y = np.zeros((self.batchSize,) + self.imageSize + (1,),dtype=\"uint8\")\n",
        "        for indexY, path in enumerate(batch_segmentation_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize, color_mode=\"grayscale\")\n",
        "            y[indexY] = np.expand_dims(img,2) # this is needed because we are trying with RGB image \n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            #y[j] -= 1. We don't need this as our labels(category id) start with 0\n",
        "\n",
        "        return x, y\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGG88EOCiE3"
      },
      "source": [
        "''' this is not needed \n",
        "# Dockerfile\n",
        "# step 1: xhost +local:docker\n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 tensorflow/tensorflow:2.4.0-gpu bash \n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 unet:1 bash \n",
        "\n",
        "FROM tensorflow/tensorflow:2.4.0-gpu\n",
        "ENV DISPLAY=$DISPLAY\n",
        "ENV CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "RUN set -eu pipefail && \\\n",
        "    export DEBIAN_FRONTEND=noninteractive && \\\n",
        "    apt-get update && \\\n",
        "    apt-get -y upgrade && \\\n",
        "    pip install Image && \\\n",
        "    apt-get -y install python-scipy python-matplotlib && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#docker build -t unet:1 .    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysNU3b5_Crjf"
      },
      "source": [
        "#UNet_model.py\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(imageSize, numClasses):\n",
        "\n",
        "    # use 3 as it's RGB\n",
        "    inputs = tf.keras.Input(shape=imageSize + (3,))\n",
        "\n",
        "    ## First half of network : Encoder : Downsampling inputs\n",
        "\n",
        "    ## Entry block\n",
        "    ## 2nd argument is kernel size\n",
        "    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x # Set aside the residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical except feature depth. filters are upsampling / downsampling slabs for unet\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "        x = tf.keras.layers.add([x, residual])\n",
        "\n",
        "        previous_block_activation = x # Set aside the next residual\n",
        "\n",
        "    # second half of the network : upsampling inputs\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = tf.keras.layers.add([x, residual]) # Add back residual\n",
        "        previous_block_activation = x # Set aside next residual\n",
        "\n",
        "    # add per pixel classification layer\n",
        "    outputs = tf.keras.layers.Conv2D(numClasses, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # define the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DN62O_C2AI"
      },
      "source": [
        "#main.py \n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#from UNet_model import create_model \n",
        "#from CreateImageDirPaths import createImageDirPaths \n",
        "#from DataSequence import MarineImages \n",
        "import PIL\n",
        "\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from PIL import ImageOps\n",
        "\n",
        "\n",
        "\n",
        "imageSize = (640, 512)\n",
        "numClasses = 7\n",
        "# sky:0, water:1, structure:2, Obstacle:3, living obstacle:4, background:5, self:6\n",
        "batchSize = 5 # This may not be possible.\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "[input_img_paths, segmentation_img_paths] = createImageDirPaths()\n",
        "\n",
        "print(\"input img paths = {0}\".format(input_img_paths))\n",
        "print(\"Type of input_img_paths = {0}\".format(type(input_img_paths)))\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = create_model(imageSize, numClasses)\n",
        "model.summary()\n",
        "\n",
        "# split image paths into a training and a validation set\n",
        "validation_samples = 5\n",
        "random.Random(15).shuffle(input_img_paths)\n",
        "random.Random(15).shuffle(segmentation_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-validation_samples]\n",
        "train_segmentation_img_paths = segmentation_img_paths[:-validation_samples]\n",
        "validation_input_img_paths = input_img_paths[-validation_samples:]\n",
        "validation_target_img_paths = segmentation_img_paths[-validation_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "marineData_training = MarineImages(batchSize, imageSize, train_input_img_paths, train_segmentation_img_paths)\n",
        "marineData_validation = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "\n",
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "\n",
        "# changing as loss was coming as NAN. this loss value gives NAN\n",
        "#model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "opt = Adam(lr=0.005) #0.001 was the default, so try a smaller one\n",
        "model.compile(optimizer=opt, loss='mean_squared_logarithmic_error')\n",
        "\n",
        "callback = [tf.keras.callbacks.ModelCheckpoint(\"uml_segmentation_color.h5\", save_best_only=True)]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 20 # CHANGE THIS LATER \n",
        "\n",
        "model.fit(marineData_training, epochs=epochs, validation_data=marineData_validation, callbacks=callback)\n",
        "\n",
        "\"\"\"Generate predictions for all images in the validation set.\"\"\"\n",
        "validation_gen = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "validation_preds = model.predict(validation_gen)  \n",
        "  \n",
        "  \n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    print (\"raw values of prediction\",validation_preds[i])\n",
        "    mask = np.argmax(validation_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    #print (\"width is {0}\". format(img.size()))\n",
        "    display(img)\n",
        "    \n",
        "# Display results for validation image #3\n",
        "i = 3\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=validation_input_img_paths[i]))\n",
        "print (\"input image displayed\")\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = PIL.ImageOps.autocontrast(load_img(validation_target_img_paths[i]))\n",
        "display(img)\n",
        "print (\"ground truth masked image displayed\")\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i) \n",
        "print (\"inference masked image displayed\") "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}