{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of UNetmodelcode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/Copy_of_UNetmodelcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fVdvrS-DZq4"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd \"drive/My Drive/PhD/IRLabeledDataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzUKnp_LBmVg"
      },
      "source": [
        "\n",
        "# CreateImageDirPaths.py\n",
        "import os\n",
        "\n",
        "inputDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized\" # directory containing input images\n",
        "segmentationImageDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized\" # directory containing output segmentation images\n",
        "\n",
        "\n",
        "def createImageDirPaths():\n",
        "\n",
        "    if (not inputDir):\n",
        "        print(\"The input dir is empty\")\n",
        "    if(not segmentationImageDir):\n",
        "        print(\"The segmentation dir is empty\")\n",
        "\n",
        "    input_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(inputDir, fName)\n",
        "            for fName in os.listdir(inputDir)\n",
        "                if fName.endswith(\".png\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    target_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(segmentationImageDir, fName)\n",
        "            for fName in os.listdir(segmentationImageDir)\n",
        "                if fName.endswith(\".png\") and not fName.startswith(\".\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Number of samples = {0}\".format(len(input_img_paths)))\n",
        "\n",
        "    for input_path, segmentation_mask_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "        print(input_path, \"|\", segmentation_mask_path)\n",
        "\n",
        "    return input_img_paths, target_img_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLkKL5vmCXYN"
      },
      "source": [
        "# DataSequence.py\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "class MarineImages(keras.utils.Sequence):\n",
        "    \"\"\" Helper to iterate over data (as Numpy array) \"\"\"\n",
        "\n",
        "    def __init__(self, batchSize, imageSize, input_img_paths, segmentation_img_paths):\n",
        "        self.batchSize = batchSize\n",
        "        self.imageSize = imageSize\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.segmentation_img_paths = segmentation_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segmentation_img_paths) // self.batchSize\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Returns tuple (input, segmentation mask) corresponding to batch #idx \"\"\"\n",
        "\n",
        "        i = idx * self.batchSize\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batchSize]\n",
        "        batch_segmentation_img_paths = self.segmentation_img_paths[i : i + self.batchSize]\n",
        "        # use 1 & uint8 as input image is grayscale. had to remove 1 as it doesn't take it here but takes it in y.  \n",
        "        x = np.zeros((self.batchSize,) + self.imageSize + (3,), dtype=\"float32\")\n",
        "        print(\"x size = {0}\".format(x.shape))\n",
        "\n",
        "        for index, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize)\n",
        "            print(\"index = {0}, path = {1}\".format(index, path))\n",
        "            print(\"target size = {0}\".format(self.imageSize))\n",
        "            print(\"img shape in PIL format = {0}\".format(img.size))\n",
        "            print(\"shape of x[{0}] = {1}\".format(index, x[index].shape))\n",
        "            x[index] = img\n",
        " \n",
        "        # use 1 & uint8 as labled image is also grayscale \n",
        "        y = np.zeros((self.batchSize,) + self.imageSize, dtype=\"uint8\")\n",
        "        for indexY, path in enumerate(batch_segmentation_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize, color_mode=\"grayscale\")\n",
        "            y[indexY] = img # without expanding dimensions\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            #y[j] -= 1. We don;t need this as our labels(category id) start with 0\n",
        "\n",
        "        return x, y\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGG88EOCiE3"
      },
      "source": [
        "''' this is not needed \n",
        "# Dockerfile\n",
        "# step 1: xhost +local:docker\n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 tensorflow/tensorflow:2.4.0-gpu bash \n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 unet:1 bash \n",
        "\n",
        "FROM tensorflow/tensorflow:2.4.0-gpu\n",
        "ENV DISPLAY=$DISPLAY\n",
        "ENV CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "RUN set -eu pipefail && \\\n",
        "    export DEBIAN_FRONTEND=noninteractive && \\\n",
        "    apt-get update && \\\n",
        "    apt-get -y upgrade && \\\n",
        "    pip install Image && \\\n",
        "    apt-get -y install python-scipy python-matplotlib && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#docker build -t unet:1 .    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysNU3b5_Crjf"
      },
      "source": [
        "#UNet_model.py\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(imageSize, numClasses):\n",
        "\n",
        "    # use 1 as it's grayscale \n",
        "    inputs = tf.keras.Input(shape=imageSize + (1,))\n",
        "\n",
        "    ## First half of network : Encoder : Downsampling inputs\n",
        "\n",
        "    ## Entry block\n",
        "    ## 2nd argument is kernel size\n",
        "    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding=\"same\", kernel_initializer = 'he_normal')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x # Set aside the residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical except feature depth. filters are upsampling / downsampling slabs for unet\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\",kernel_initializer = 'he_normal')(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\",kernel_initializer = 'he_normal')(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\",kernel_initializer = 'he_normal')(previous_block_activation)\n",
        "        x = tf.keras.layers.add([x, residual])\n",
        "\n",
        "        previous_block_activation = x # Set aside the next residual\n",
        "\n",
        "    # second half of the network : upsampling inputs\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, padding=\"same\",kernel_initializer = 'he_normal')(residual)\n",
        "        x = tf.keras.layers.add([x, residual]) # Add back residual\n",
        "        previous_block_activation = x # Set aside next residual\n",
        "\n",
        "    # add per pixel classification layer\n",
        "    outputs = tf.keras.layers.Conv2D(numClasses, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # define the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DN62O_C2AI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab5ac2c1-264d-4cdd-f523-aa2cc12eb6aa"
      },
      "source": [
        "#main.py \n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#from UNet_model import create_model \n",
        "#from CreateImageDirPaths import createImageDirPaths \n",
        "#from DataSequence import MarineImages \n",
        "import PIL\n",
        "\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from PIL import ImageOps\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "imageSize = (640, 512)\n",
        "numClasses = 7\n",
        "# sky:0, water:1, structure:2, Obstacle:3, living obstacle:4, background:5, self:6\n",
        "batchSize = 5 # This may not be possible.\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "[input_img_paths, segmentation_img_paths] = createImageDirPaths()\n",
        "\n",
        "print(\"input img paths = {0}\".format(input_img_paths))\n",
        "print(\"Type of input_img_paths = {0}\".format(type(input_img_paths)))\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = create_model(imageSize, numClasses)\n",
        "model.summary()\n",
        "\n",
        "# split image paths into a training and a validation set\n",
        "validation_samples = 5\n",
        "random.Random(15).shuffle(input_img_paths)\n",
        "random.Random(15).shuffle(segmentation_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-validation_samples]\n",
        "train_segmentation_img_paths = segmentation_img_paths[:-validation_samples]\n",
        "validation_input_img_paths = input_img_paths[-validation_samples:]\n",
        "validation_target_img_paths = segmentation_img_paths[-validation_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "marineData_training = MarineImages(batchSize, imageSize, train_input_img_paths, train_segmentation_img_paths)\n",
        "marineData_validation = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "\n",
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callback = [tf.keras.callbacks.ModelCheckpoint(\"uml_segmentation.h5\", save_best_only=True)]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 10\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "model.fit(marineData_training, epochs=epochs, validation_data=marineData_validation, callbacks=[early_stopping])\n",
        "\n",
        "\"\"\"Generate predictions for all images in the validation set.\"\"\"\n",
        "validation_gen = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "validation_preds = model.predict(validation_gen)  \n",
        "  \n",
        "  \n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    print (\"raw values of prediction\",validation_preds[i])\n",
        "    mask = np.argmax(validation_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    #print (\"width is {0}\". format(img.size()))\n",
        "    display(img)\n",
        "    \n",
        "# Display results for validation image #3\n",
        "i = 3\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=validation_input_img_paths[i]))\n",
        "print (\"input image displayed\")\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = PIL.ImageOps.autocontrast(load_img(validation_target_img_paths[i]))\n",
        "display(img)\n",
        "print (\"ground truth masked image displayed\")\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i) \n",
        "print (\"inference masked image displayed\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples = 62\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388082.940481_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388082.940481_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388099.294270_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388099.294270_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388115.497458_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388115.497458_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388165.895730_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388165.895730_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388188.478616_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388188.478616_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388213.024487_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388233.503927_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388233.503927_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388257.753172_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388257.753172_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388265.894147_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388265.894147_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388285.976467_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388285.976467_label_ground-truth_resized.png\n",
            "input img paths = ['/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388082.940481_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388099.294270_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388115.497458_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388165.895730_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388188.478616_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388233.503927_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388257.753172_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388265.894147_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388285.976467_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388290.143886_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388298.142353_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388318.501611_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388322.605193_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388359.104736_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388375.333846_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388377.936287_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388388.824583_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388393.583052_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388396.583695_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388403.973998_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388406.499554_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388414.999819_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388452.556152_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388501.080463_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388569.911970_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388638.518627_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388646.601730_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388658.768223_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388691.096958_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388699.327823_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388731.574273_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388743.715882_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388747.882511_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388751.716032_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388754.074375_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388780.130981_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388796.213713_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388913.212096_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388933.294796_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388949.402915_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388965.627199_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388997.922736_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389058.484265_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389090.649281_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389110.873336_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389131.086094_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389155.289434_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389167.334687_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389187.480874_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389207.562923_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389248.003933_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389256.028548_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389312.560653_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389332.830747_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389461.915059_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389490.081358_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389607.073981_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389635.468905_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389716.488182_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389736.608939_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389764.974272_resized.png']\n",
            "Type of input_img_paths = <class 'list'>\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 640, 512, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 320, 256, 32) 320         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 320, 256, 32) 128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 320, 256, 32) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 320, 256, 32) 0           activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_18 (SeparableC (None, 320, 256, 64) 2400        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 320, 256, 64) 256         separable_conv2d_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 320, 256, 64) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_19 (SeparableC (None, 320, 256, 64) 4736        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 320, 256, 64) 256         separable_conv2d_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 160, 128, 64) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 160, 128, 64) 2112        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 160, 128, 64) 0           max_pooling2d_9[0][0]            \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 160, 128, 64) 0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_20 (SeparableC (None, 160, 128, 128 8896        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 160, 128, 128 512         separable_conv2d_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 160, 128, 128 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_21 (SeparableC (None, 160, 128, 128 17664       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 160, 128, 128 512         separable_conv2d_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 80, 64, 128)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 80, 64, 128)  8320        add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 80, 64, 128)  0           max_pooling2d_10[0][0]           \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 80, 64, 128)  0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_22 (SeparableC (None, 80, 64, 256)  34176       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 80, 64, 256)  1024        separable_conv2d_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 80, 64, 256)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_23 (SeparableC (None, 80, 64, 256)  68096       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 80, 64, 256)  1024        separable_conv2d_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 40, 32, 256)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 40, 32, 256)  33024       add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 40, 32, 256)  0           max_pooling2d_11[0][0]           \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 40, 32, 256)  0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_24 (Conv2DTran (None, 40, 32, 256)  590080      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 40, 32, 256)  1024        conv2d_transpose_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 40, 32, 256)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_25 (Conv2DTran (None, 40, 32, 256)  590080      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 40, 32, 256)  1024        conv2d_transpose_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_25 (UpSampling2D) (None, 80, 64, 256)  0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_24 (UpSampling2D) (None, 80, 64, 256)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 80, 64, 256)  65792       up_sampling2d_25[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 80, 64, 256)  0           up_sampling2d_24[0][0]           \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 80, 64, 256)  0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_26 (Conv2DTran (None, 80, 64, 128)  295040      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 80, 64, 128)  512         conv2d_transpose_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 80, 64, 128)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_27 (Conv2DTran (None, 80, 64, 128)  147584      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 80, 64, 128)  512         conv2d_transpose_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_27 (UpSampling2D) (None, 160, 128, 256 0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_26 (UpSampling2D) (None, 160, 128, 128 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 160, 128, 128 32896       up_sampling2d_27[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 160, 128, 128 0           up_sampling2d_26[0][0]           \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 160, 128, 128 0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_28 (Conv2DTran (None, 160, 128, 64) 73792       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 160, 128, 64) 256         conv2d_transpose_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 160, 128, 64) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_29 (Conv2DTran (None, 160, 128, 64) 36928       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 160, 128, 64) 256         conv2d_transpose_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_29 (UpSampling2D) (None, 320, 256, 128 0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_28 (UpSampling2D) (None, 320, 256, 64) 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 320, 256, 64) 8256        up_sampling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 320, 256, 64) 0           up_sampling2d_28[0][0]           \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 320, 256, 64) 0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_30 (Conv2DTran (None, 320, 256, 32) 18464       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 320, 256, 32) 128         conv2d_transpose_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 320, 256, 32) 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_31 (Conv2DTran (None, 320, 256, 32) 9248        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 320, 256, 32) 128         conv2d_transpose_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_31 (UpSampling2D) (None, 640, 512, 64) 0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_30 (UpSampling2D) (None, 640, 512, 32) 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 640, 512, 32) 2080        up_sampling2d_31[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 640, 512, 32) 0           up_sampling2d_30[0][0]           \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 640, 512, 7)  2023        add_27[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,059,559\n",
            "Trainable params: 2,055,783\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n",
            "x size = (5, 640, 512, 3)\n",
            "index = 0, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[0] = (640, 512, 3)\n",
            "index = 1, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388233.503927_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[1] = (640, 512, 3)\n",
            "index = 2, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388965.627199_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[2] = (640, 512, 3)\n",
            "index = 3, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389312.560653_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[3] = (640, 512, 3)\n",
            "index = 4, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389167.334687_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[4] = (640, 512, 3)\n",
            "Epoch 1/10\n",
            "x size = (5, 640, 512, 3)\n",
            "index = 0, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389607.073981_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[0] = (640, 512, 3)\n",
            "index = 1, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388298.142353_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[1] = (640, 512, 3)\n",
            "index = 2, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389207.562923_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[2] = (640, 512, 3)\n",
            "index = 3, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388658.768223_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[3] = (640, 512, 3)\n",
            "index = 4, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389490.081358_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[4] = (640, 512, 3)\n",
            "x size = (5, 640, 512, 3)\n",
            "index = 0, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389110.873336_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[0] = (640, 512, 3)\n",
            "index = 1, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388638.518627_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[1] = (640, 512, 3)\n",
            "index = 2, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388396.583695_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[2] = (640, 512, 3)\n",
            "index = 3, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388796.213713_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0be1bfc306a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarineData_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarineData_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\"\"\"Generate predictions for all images in the validation set.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m:  No algorithm worked!\n\t [[node model_3/conv2d_27/Conv2D (defined at <ipython-input-14-0be1bfc306a8>:60) ]] [Op:__inference_train_function_16576]\n\nFunction call stack:\ntrain_function\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "shape of x[3] = (640, 512, 3)\n",
            "index = 4, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388403.973998_resized.png\n",
            "target size = (640, 512)\n",
            "img shape in PIL format = (512, 640)\n",
            "shape of x[4] = (640, 512, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}