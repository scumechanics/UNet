{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNetmodelcode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtviD09AKgeAywAm1ZEHrU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/UNet/blob/main/UNetmodelcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fVdvrS-DZq4",
        "outputId": "083cd330-af88-47e0-8ec7-55186db46cc4"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cd \"drive/My Drive/PhD/IRLabeledDataset\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzUKnp_LBmVg"
      },
      "source": [
        "\n",
        "# CreateImageDirPaths.py\n",
        "import os\n",
        "\n",
        "inputDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized\" # directory containing input images\n",
        "segmentationImageDir = \"/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized\" # directory containing output segmentation images\n",
        "\n",
        "\n",
        "def createImageDirPaths():\n",
        "\n",
        "    if (not inputDir):\n",
        "        print(\"The input dir is empty\")\n",
        "    if(not segmentationImageDir):\n",
        "        print(\"The segmentation dir is empty\")\n",
        "\n",
        "    input_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(inputDir, fName)\n",
        "            for fName in os.listdir(inputDir)\n",
        "                if fName.endswith(\".png\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    target_img_paths = sorted (\n",
        "        [\n",
        "            os.path.join(segmentationImageDir, fName)\n",
        "            for fName in os.listdir(segmentationImageDir)\n",
        "                if fName.endswith(\".png\") and not fName.startswith(\".\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"Number of samples = {0}\".format(len(input_img_paths)))\n",
        "\n",
        "    for input_path, segmentation_mask_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "        print(input_path, \"|\", segmentation_mask_path)\n",
        "\n",
        "    return input_img_paths, target_img_paths"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLkKL5vmCXYN"
      },
      "source": [
        "# DataSequence.py\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "class MarineImages(keras.utils.Sequence):\n",
        "    \"\"\" Helper to iterate over data (as Numpy array) \"\"\"\n",
        "\n",
        "    def __init__(self, batchSize, imageSize, input_img_paths, segmentation_img_paths):\n",
        "        self.batchSize = batchSize\n",
        "        self.imageSize = imageSize\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.segmentation_img_paths = segmentation_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.segmentation_img_paths) // self.batchSize\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Returns tuple (input, segmentation mask) corresponding to batch #idx \"\"\"\n",
        "\n",
        "        i = idx * self.batchSize\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batchSize]\n",
        "        batch_segmentation_img_paths = self.segmentation_img_paths[i : i + self.batchSize]\n",
        "        # use 1 & uint8 as input image is grayscale. had to remove 1 as it doesn't take it here but takes it in y.  \n",
        "        x = np.zeros((self.batchSize,) + self.imageSize, dtype=\"uint8\")\n",
        "        print(\"x size = {0}\".format(x.shape))\n",
        "\n",
        "        for index, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize, color_mode = \"grayscale\")\n",
        "            print(\"index = {0}, path = {1}\".format(index, path))\n",
        "            print(\"target size = {0}\".format(self.imageSize))\n",
        "            print(\"img shape = {0}\".format(img.size))\n",
        "            print(\"shape of x[{0}] = {1}\".format(index, x[index].shape))\n",
        "            x[index] = img\n",
        " \n",
        "        # use 1 & uint8 as labled image is also grayscale \n",
        "        y = np.zeros((self.batchSize,) + self.imageSize, dtype=\"uint8\")\n",
        "        for indexY, path in enumerate(batch_segmentation_img_paths):\n",
        "            img = load_img(path, target_size=self.imageSize, color_mode=\"grayscale\")\n",
        "            y[indexY] = img # without expanding dimensions\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            #y[j] -= 1. We don;t need this as our labels(category id) start with 0\n",
        "\n",
        "        return x, y\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGG88EOCiE3"
      },
      "source": [
        "''' this is not needed \n",
        "# Dockerfile\n",
        "# step 1: xhost +local:docker\n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 tensorflow/tensorflow:2.4.0-gpu bash \n",
        "# docker run -u $(id -u):$(id -g) -it --rm -e DISPLAY=${DISPLAY} --gpus all -v $(pwd)/:/UNet:consistent -v /home/shailesh/Work/catkin_ws/src/simplepackage/src/IRLabeledDataset/:/IRLabeledDataset:consistent -v /tmp/.X11-unix:/tmp/.X11-unix -v ${HOME}/.Xauthority:/home/xterm/.Xauthority --hostname $(hostname) -p 0.0.0.0:6006:6006 unet:1 bash \n",
        "\n",
        "FROM tensorflow/tensorflow:2.4.0-gpu\n",
        "ENV DISPLAY=$DISPLAY\n",
        "ENV CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "RUN set -eu pipefail && \\\n",
        "    export DEBIAN_FRONTEND=noninteractive && \\\n",
        "    apt-get update && \\\n",
        "    apt-get -y upgrade && \\\n",
        "    pip install Image && \\\n",
        "    apt-get -y install python-scipy python-matplotlib && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "#docker build -t unet:1 .    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysNU3b5_Crjf"
      },
      "source": [
        "#UNet_model.py\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(imageSize, numClasses):\n",
        "\n",
        "    # use 1 as it's grayscale \n",
        "    inputs = tf.keras.Input(shape=imageSize + (1,))\n",
        "\n",
        "    ## First half of network : Encoder : Downsampling inputs\n",
        "\n",
        "    ## Entry block\n",
        "    ## 2nd argument is kernel size\n",
        "    x = tf.keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x # Set aside the residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical except feature depth. filters are upsampling / downsampling slabs for unet\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "        x = tf.keras.layers.add([x, residual])\n",
        "\n",
        "        previous_block_activation = x # Set aside the next residual\n",
        "\n",
        "    # second half of the network : upsampling inputs\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # project residual\n",
        "        residual = tf.keras.layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = tf.keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = tf.keras.layers.add([x, residual]) # Add back residual\n",
        "        previous_block_activation = x # Set aside next residual\n",
        "\n",
        "    # add per pixel classification layer\n",
        "    outputs = tf.keras.layers.Conv2D(numClasses, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # define the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9DN62O_C2AI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bf75a43-81be-498d-9826-58f54ac8b2e2"
      },
      "source": [
        "#main.py \n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#from UNet_model import create_model \n",
        "#from CreateImageDirPaths import createImageDirPaths \n",
        "#from DataSequence import MarineImages \n",
        "import PIL\n",
        "\n",
        "import random\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from PIL import ImageOps\n",
        "\n",
        "\n",
        "\n",
        "imageSize = (640, 512)\n",
        "numClasses = 7\n",
        "# sky:0, water:1, structure:2, Obstacle:3, living obstacle:4, background:5, self:6\n",
        "batchSize = 5 # This may not be possible.\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "[input_img_paths, segmentation_img_paths] = createImageDirPaths()\n",
        "\n",
        "print(\"input img paths = {0}\".format(input_img_paths))\n",
        "print(\"Type of input_img_paths = {0}\".format(type(input_img_paths)))\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = create_model(imageSize, numClasses)\n",
        "model.summary()\n",
        "\n",
        "# split image paths into a training and a validation set\n",
        "validation_samples = 5\n",
        "random.Random(15).shuffle(input_img_paths)\n",
        "random.Random(15).shuffle(segmentation_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-validation_samples]\n",
        "train_segmentation_img_paths = segmentation_img_paths[:-validation_samples]\n",
        "validation_input_img_paths = input_img_paths[-validation_samples:]\n",
        "validation_target_img_paths = segmentation_img_paths[-validation_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "marineData_training = MarineImages(batchSize, imageSize, train_input_img_paths, train_segmentation_img_paths)\n",
        "marineData_validation = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "\n",
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callback = [tf.keras.callbacks.ModelCheckpoint(\"uml_segmentation.h5\", save_best_only=True)]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 10\n",
        "model.fit(marineData_training, epochs=epochs, validation_data=marineData_validation, callbacks=callback)\n",
        "\n",
        "\"\"\"Generate predictions for all images in the validation set.\"\"\"\n",
        "validation_gen = MarineImages(batchSize, imageSize, validation_input_img_paths, validation_target_img_paths)\n",
        "validation_preds = model.predict(validation_gen)  \n",
        "  \n",
        "  \n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(validation_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "    \n",
        "# Display results for validation image #3\n",
        "i = 3\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=validation_input_img_paths[i]))\n",
        "print (\"input image displayed\")\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = PIL.ImageOps.autocontrast(load_img(validation_target_img_paths[i]))\n",
        "display(img)\n",
        "print (\"ground truth masked image displayed\")\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i) \n",
        "print (\"inference masked image displayed\") "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples = 62\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388082.940481_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388082.940481_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388099.294270_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388099.294270_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388115.497458_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388115.497458_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388165.895730_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388165.895730_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388188.478616_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388188.478616_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388213.024487_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388233.503927_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388233.503927_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388257.753172_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388257.753172_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388265.894147_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388265.894147_label_ground-truth_resized.png\n",
            "/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388285.976467_resized.png | /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_masks_4_resized/1603388285.976467_label_ground-truth_resized.png\n",
            "input img paths = ['/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388082.940481_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388099.294270_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388115.497458_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388165.895730_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388188.478616_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388233.503927_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388257.753172_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388265.894147_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388285.976467_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388290.143886_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388298.142353_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388318.501611_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388322.605193_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388359.104736_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388375.333846_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388377.936287_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388388.824583_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388393.583052_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388396.583695_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388403.973998_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388406.499554_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388414.999819_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388452.556152_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388501.080463_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388569.911970_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388638.518627_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388646.601730_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388658.768223_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388691.096958_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388699.327823_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388731.574273_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388743.715882_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388747.882511_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388751.716032_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388754.074375_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388780.130981_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388796.213713_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388913.212096_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388933.294796_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388949.402915_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388965.627199_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388997.922736_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389058.484265_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389090.649281_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389110.873336_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389131.086094_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389155.289434_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389167.334687_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389187.480874_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389207.562923_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389248.003933_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389256.028548_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389312.560653_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389332.830747_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389461.915059_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389490.081358_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389607.073981_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389635.468905_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389716.488182_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389736.608939_resized.png', '/content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603389764.974272_resized.png']\n",
            "Type of input_img_paths = <class 'list'>\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 640, 512, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 320, 256, 32) 320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 320, 256, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 320, 256, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 320, 256, 32) 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d (SeparableConv (None, 320, 256, 64) 2400        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 320, 256, 64) 256         separable_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 320, 256, 64) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 320, 256, 64) 4736        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 320, 256, 64) 256         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 160, 128, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 160, 128, 64) 2112        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 160, 128, 64) 0           max_pooling2d[0][0]              \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 160, 128, 64) 0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 160, 128, 128 8896        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 160, 128, 128 512         separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 160, 128, 128 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 160, 128, 128 17664       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 160, 128, 128 512         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 80, 64, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 80, 64, 128)  8320        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 80, 64, 128)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 80, 64, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 80, 64, 256)  34176       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 80, 64, 256)  1024        separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 80, 64, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 80, 64, 256)  68096       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 80, 64, 256)  1024        separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 40, 32, 256)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 40, 32, 256)  33024       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 40, 32, 256)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 40, 32, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 40, 32, 256)  590080      activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 40, 32, 256)  1024        conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 40, 32, 256)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 40, 32, 256)  590080      activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 40, 32, 256)  1024        conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 80, 64, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 80, 64, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 80, 64, 256)  65792       up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 80, 64, 256)  0           up_sampling2d[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 80, 64, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 80, 64, 128)  295040      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 80, 64, 128)  512         conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 80, 64, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 80, 64, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 80, 64, 128)  512         conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 160, 128, 256 0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 160, 128, 128 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 160, 128, 128 32896       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 160, 128, 128 0           up_sampling2d_2[0][0]            \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 160, 128, 128 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 160, 128, 64) 73792       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 160, 128, 64) 256         conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 160, 128, 64) 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 160, 128, 64) 36928       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 160, 128, 64) 256         conv2d_transpose_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 320, 256, 128 0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 320, 256, 64) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 320, 256, 64) 8256        up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 320, 256, 64) 0           up_sampling2d_4[0][0]            \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 320, 256, 64) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 320, 256, 32) 18464       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 320, 256, 32) 128         conv2d_transpose_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 320, 256, 32) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 320, 256, 32) 9248        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 320, 256, 32) 128         conv2d_transpose_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 640, 512, 64) 0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 640, 512, 32) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 640, 512, 32) 2080        up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 640, 512, 32) 0           up_sampling2d_6[0][0]            \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 640, 512, 7)  2023        add_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 2,059,559\n",
            "Trainable params: 2,055,783\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n",
            "x size = (5, 640, 512, 1)\n",
            "index = 0, path = /content/drive/MyDrive/PhD/IRLabeledDataset/ir_train_images_4_resized/1603388213.024487_resized.png\n",
            "target size = (640, 512)\n",
            "img shape = (512, 640)\n",
            "shape of x[0] = (640, 512, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a96f3488b7c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Train the model, doing validation at the end of each epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarineData_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarineData_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\"\"\"Generate predictions for all images in the validation set.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
            "\u001b[0;32m<ipython-input-4-bc8a509084a3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img shape = {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of x[{0}] = {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# use 1 & uint8 as labled image is also grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (640,512) into shape (640,512,1)"
          ]
        }
      ]
    }
  ]
}